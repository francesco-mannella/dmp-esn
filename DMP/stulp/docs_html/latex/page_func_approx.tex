\hypertarget{page_func_approx_sec_fa}{}\subsection{Function Approximation}\label{page_func_approx_sec_fa}
This module implements a set of function approximators, i.\+e. supervised learning algorithms that are trained with demonstration pairs input/target, after which they make predictions for new inputs. For simplicity, this module implements only batch learning (not incremental), and does not allow trained function approximators to be retrained.

The two main functions are Function\+Approximator\+::train, which takes a set of inputs and corresponding targets, and Function\+Approximator\+::predict, which makes predictions for novel inputs.\hypertarget{page_func_approx_sec_fa_metaparameters}{}\subsubsection{Meta\+Parameters and Model\+Parameters}\label{page_func_approx_sec_fa_metaparameters}
In this module, algorithmic parameters are called Meta\+Parameters, and the parameters of the model when the function approximator has been trained are called Model\+Parameters. The rationale for this is that an untrained function approximator can be entirely reconstructed if its Meta\+Parameters are known; this is useful for saving to file and making copies. A trained function approximator can be compeletely reconstructed given only its Model\+Parameters.

The life-\/cycle of a function approximator is as follows\+:

{\bfseries 1}. {\bfseries Initialization\+:} The function approximator is initialized by calling the constructor with the Meta\+Parameters. Its Model\+Parameters are set to N\+U\+L\+L, indicating that the model is untrained.

{\bfseries 2}. {\bfseries Training\+:} Function\+Approximator\+::train is called, which performs the conversion\+: $ \mbox{train}: \mbox{MetaParameters} \times \mbox{Inputs} \times \mbox{Targets} \mapsto \mbox{ModelParameters} $

{\bfseries 3}. {\bfseries Prediction\+:} Function\+Approximator\+::predict is called, which performs the conversion\+: $ \mbox{predict}: \mbox{ModelParameters} \times \mbox{Input} \mapsto \mbox{Output}$

{\itshape Remark}. Function\+Approximator\+::train in Step {\bfseries 2}. may only be called once. If you explicitly want to retrain the function approximator with novel input/target data call Function\+Approximator\+::re\+Train() instead.

{\itshape Remark}. During the initialization, Model\+Parameters may also be passed to the constructor. This means that an already trained function approximator is initialized. Step {\bfseries 2}. above is thus skipped.\hypertarget{page_func_approx_sec_fa_changing_modelparameters}{}\subsubsection{Changing the Model\+Parameters of a Function\+Approximator}\label{page_func_approx_sec_fa_changing_modelparameters}
The user should not be allowed to set the Model\+Parameters of a trained function approximator directly. Hence, Function\+Approximator\+::set\+Model\+Parameters is protected. However, in order to change the values inside the model parameters (for instance when optimizing them), the user may call Model\+Parameters\+::get\+Parameter\+Vector\+Selected and Model\+Parameters\+::set\+Parameter\+Vector\+Selected it inherits these functions from Parameterizable). These take a vector of doubles, check if the vector has the right size, and get/set the Model\+Parameters accordingly.

Function approximators often have different types of model parameters. For instance, the model parameters of Locally Weighted Regression (Function\+Approximator\+L\+W\+R) represent the centers and widths of the basis functions, as well as the slopes of the line segments. If you only want to get/set the slopes when calling Model\+Parameters\+::get\+Parameter\+Vector\+Selected and Model\+Parameters\+::set\+Parameter\+Vector\+Selected, you must use Model\+Parameters\+::set\+Selected\+Parameters(const std\+::set$<$std\+::string$>$\& selected\+\_\+values\+\_\+labels), for instance as follows\+:


\begin{DoxyCode}
std::set<std::string> selected;
selected.insert(\textcolor{stringliteral}{"slopes"});
model\_parameters.setSelectedParameters(selected);
Eigen::VectorXd values;
model\_parameters.getParameterVectorSelected(values);
\textcolor{comment}{// "values" now only contains the slopes of the line segments}

selected.clear();
selected.insert(\textcolor{stringliteral}{"centers"});
selected.insert(\textcolor{stringliteral}{"slopes"});
model\_parameters.setSelectedParameters(selected);
model\_parameters.getParameterVectorSelected(values);
\textcolor{comment}{// "values" now contains the centers of the basis functions AND slopes of the line segments}
\end{DoxyCode}


The rationale behind this implementation is that optimizers (such as evolution strategies) should not have to care about whether a particular set of model parameters contains centers, widths or slopes. Therefore, these different types of parameters are provided in one vector without semantics, and the generic interface is provided by the Parameterizable class.

Classes that inherit from Parameterizable (such as all Model\+Parameters and Function\+Approximator subclasses, must implement the pure virtual methods Parameterizable\+::get\+Parameter\+Vector\+All Parameterizable\+::set\+Parameter\+Vector\+All and Parameterizable\+::get\+Parameter\+Vector\+Mask. Which gets/sets all the possible parameters in one vector, and a mask specifying the semantics of each value in the vector. The work of setting/getting the selected parameters (and normalizing them) is done in the Parameterizable class itself. This approach is a slightly longer run-\/time than doing the work in the subclasses, but it leads to more legible and robust code (less code duplication).\hypertarget{page_func_approx_sec_caching_basisfunctions}{}\subsubsection{Caching of basis functions}\label{page_func_approx_sec_caching_basisfunctions}
If the parameters of the basis functions (centers and widths of the kernels) do not change often, you can cache the basis function activations by calling Model\+Parameters\+::set\+\_\+caching(true). This can lead to speed improvements because the activations are not computed over and over again. This function only makes senses if the inputs remain the same, i.\+e. this is not the case when running on a real robot.

The reason why caching is implemented in Model\+Parameters, and not in Function\+Approximator is because Model\+Parameters knows which parts of the Model\+Parameters change the basis function activations, and which do not (for instance in R\+B\+F\+N, the widths and centers change the basis function activations, but the weights do not). 